{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a43d9c1",
   "metadata": {},
   "source": [
    "# Fine‑tuning **GPT‑4.1 nano** for <$0.50  \n",
    "\n",
    "This notebook shows a minimal, low‑cost fine‑tuning workflow that trains **GPT‑4.1 nano** on 100 short jokes (~15 K tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d52b7212",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet openai tiktoken pandas datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f735b3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, random, tiktoken, pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from datasets import load_dataset\n",
    "import openai\n",
    "\n",
    "# Set OpenAI API key\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6ae6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total jokes loaded: 52000\n",
      "[(\"Why couldn't the toilet paper cross the road\", 'It got stuck in a crack'), (\"Dadjoked a cashier I was out with some friends, we were grabbing some food at a local coffee shop.  Their prices were fucking sweet, like three bucks for a sandwich.  Anyway, I placed by order:  -$4. 50 for a grilled cheese (heavenly) - -$3. 00 for a small shake - -$0. 60 tax - The cashier nods and says,  Thank you, that'll be $8\", \"10   I replied,  It's about to be *ea*-ten   I'm pretty sure they spit in my food\"), ('The future, the present and the past walked into a bar,', 'Things got a little tense.'), ('My daughter got engaged to a Russian guy and her wedding is in a few weeks', 'I’m just really worried about the Soviet Union'), ('6:30 is the best time of day', 'Hands down')]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "# i want to use ysharma/short_jokes but don't work because moderation restrictions i get \n",
    "# The job failed due to an unsafe training file. This training \n",
    "# file was blocked by our moderation system because it contains\n",
    "# too many examples that violate OpenAI's usage policies, or because \n",
    "# it attempts to create model outputs that violate OpenAI's usage policies.\n",
    "# so i will use shuttie/dadjokes instead\n",
    "ds = load_dataset('shuttie/dadjokes', split='train')\n",
    "print(\"Total jokes loaded:\", len(ds))\n",
    "# Sample 50 jokes for demonstration\n",
    "sampled = random.sample(list(zip(ds[\"question\"], ds[\"response\"])), 50)\n",
    "print(sampled[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b07d246a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'role': 'system', 'content': 'You are a witty assistant that answers with a short, family‑friendly dad joke.'}, {'role': 'user', 'content': \"Why couldn't the toilet paper cross the road\"}, {'role': 'assistant', 'content': 'It got stuck in a crack'}]}\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_PROMPT = \"You are a witty assistant that answers with a short, family‑friendly dad joke.\"\n",
    "\n",
    "train_examples = [\n",
    "    {\"messages\":[\n",
    "        {\"role\":\"system\",\"content\":SYSTEM_PROMPT},\n",
    "        {\"role\":\"user\",\"content\":q.strip()},\n",
    "        {\"role\":\"assistant\",\"content\":a.strip()}\n",
    "    ]} for q,a in sampled\n",
    "]\n",
    "\n",
    "print(train_examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c77d091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated tokens: 2273\n",
      "≈ $0.0034 total training cost\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "\n",
    "token_total = 0\n",
    "for ex in train_examples:\n",
    "    for m in ex[\"messages\"]:\n",
    "        token_total += len(enc.encode(m[\"content\"]))\n",
    "\n",
    "print(f\"Estimated tokens: {token_total}\")\n",
    "\n",
    "USD_PER_M_TOKENS_TRAIN = 1.50 \n",
    "train_cost = (token_total / 1_000_000) * USD_PER_M_TOKENS_TRAIN\n",
    "print(f\"≈ ${train_cost:.4f} total training cost\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d57f14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote jokes_gpt41nano.jsonl\n"
     ]
    }
   ],
   "source": [
    "jsonl_path = \"jokes_gpt41nano.jsonl\"\n",
    "with open(jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for ex in train_examples:\n",
    "        f.write(json.dumps(ex) + \"\\n\")\n",
    "\n",
    "print(\"Wrote\", jsonl_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4012fb78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file: file-1ZU1P8Qm1NA7JqWoVqLpD4\n",
      "Job started: ftjob-WLpCJIma9qg3jEn8vc5Q1Rhn\n"
     ]
    }
   ],
   "source": [
    "# Upload file\n",
    "file_obj = openai.files.create(\n",
    "    file=open(jsonl_path, \"rb\"),\n",
    "    purpose=\"fine-tune\"\n",
    ")\n",
    "print(\"Uploaded file:\", file_obj.id)\n",
    "\n",
    "# Start fine‑tuning job (single epoch keeps cost tiny; you can raise if desired)\n",
    "job = openai.fine_tuning.jobs.create(\n",
    "    training_file=file_obj.id,\n",
    "    model=\"gpt-4.1-nano-2025-04-14\",\n",
    "    hyperparameters={\n",
    "        \"n_epochs\": 1\n",
    "    }\n",
    ")\n",
    "print(\"Job started:\", job.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ef74afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: validating_files\n",
      "Status: validating_files\n",
      "Status: validating_files\n",
      "Status: validating_files\n",
      "Status: validating_files\n",
      "Status: validating_files\n",
      "Status: validating_files\n",
      "Status: validating_files\n",
      "Status: validating_files\n",
      "Status: validating_files\n",
      "Status: validating_files\n",
      "Status: validating_files\n",
      "Status: validating_files\n",
      "Status: validating_files\n",
      "Status: validating_files\n",
      "Status: validating_files\n",
      "Status: validating_files\n",
      "Status: validating_files\n",
      "Status: validating_files\n",
      "Status: validating_files\n",
      "Status: validating_files\n",
      "Status: validating_files\n",
      "Status: validating_files\n",
      "Status: validating_files\n",
      "Status: validating_files\n",
      "Status: validating_files\n",
      "Status: validating_files\n",
      "Status: validating_files\n",
      "Status: validating_files\n",
      "Status: validating_files\n",
      "Status: validating_files\n",
      "Status: validating_files\n",
      "Status: validating_files\n",
      "Status: validating_files\n",
      "Status: validating_files\n",
      "Status: validating_files\n",
      "Status: validating_files\n",
      "Status: validating_files\n",
      "Status: validating_files\n",
      "Status: validating_files\n",
      "Status: validating_files\n",
      "Status: validating_files\n",
      "Status: validating_files\n",
      "Status: validating_files\n",
      "Status: validating_files\n",
      "Status: validating_files\n",
      "Status: validating_files\n",
      "Status: validating_files\n",
      "Status: validating_files\n",
      "Status: validating_files\n",
      "Status: validating_files\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: succeeded\n"
     ]
    }
   ],
   "source": [
    "import time, sys\n",
    "while True:\n",
    "    job_status = openai.fine_tuning.jobs.retrieve(job.id)\n",
    "    print(\"Status:\", job_status.status)\n",
    "    if job_status.status in (\"succeeded\", \"failed\", \"cancelled\"):\n",
    "        break\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ce0b28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine‑tuned model id: ft:gpt-4.1-nano-2025-04-14:personal::By6Rub20\n",
      "My computer kept freezing, so I ran the defrost program\n"
     ]
    }
   ],
   "source": [
    "MODEL_ID = job_status.fine_tuned_model\n",
    "print(\"Fine‑tuned model id:\", MODEL_ID)\n",
    "\n",
    "resp = openai.chat.completions.create(\n",
    "    model=MODEL_ID,\n",
    "    messages=[\n",
    "        {\"role\":\"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\":\"user\", \"content\": \"Tell me a short joke about computers.\"}\n",
    "    ]\n",
    ")\n",
    "print(resp.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
